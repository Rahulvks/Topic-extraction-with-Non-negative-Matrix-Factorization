{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import plotly\n",
    "\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import Counter\n",
    "from scipy.misc import imread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "import plotly.offline as py\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['href','br'])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/Users/dsg281/Downloads/Dubai_now_log/Dubai_Now_Logs_English.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(data)):\n",
    "    \n",
    "    data.iloc[idx]['Questions'] = [word for word in tokenizer.tokenize(data.iloc[idx]['Questions'].lower()) if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Questions'] = data['Questions'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = [value[0] for value in data.iloc[0:].values]\n",
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(train_)\n",
    "texts = [bigram[line] for line in train_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = gensim.corpora.Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,chunksize=100,alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = 10);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(lda, num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"/Users/dsg281/Downloads/Dubai_now_log/Dubai_Now_Logs_English.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic:\", (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "def tfidf_vectorizer(documents,total_features):\n",
    "\n",
    "    #  TFIDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=total_features,ngram_range=(2, 3),stop_words='english',analyzer='word',lowercase=True)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    return tfidf_vectorizer,tfidf,tfidf_feature_names\n",
    "\n",
    "def count_vectorizer(documents,total_features):\n",
    "\n",
    "    #  Count Vectorizer\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, ngram_range=(2, 3),max_features=total_features, stop_words='english')\n",
    "    tf = tf_vectorizer.fit_transform(documents)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    return tf_vectorizer,tf,tf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = 200\n",
    "num_topic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, tfidf, tfidf_feature_names = tfidf_vectorizer(data['Questions'],total_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer, tf, tf_feature_names = count_vectorizer(data['Questions'],total_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = LatentDirichletAllocation(n_topics=num_topic, max_iter=50, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf)\n",
    "no_top_words = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(model_lda, tfidf_feature_names, no_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_op = pyLDAvis.sklearn.prepare(model_lda,tfidf,tfidf_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(data_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try3 - Final Select NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=50,ngram_range=(1,2),stop_words='english')\n",
    "x_counts = vectorizer.fit_transform(data['Questions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "x_tfidf = transformer.fit_transform(x_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=num_topics, init='nndsvd');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(xtfidf_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-3 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nmf_topics(model, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
